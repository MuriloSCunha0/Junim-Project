"""
ğŸ§ª TESTE DO SISTEMA UNIVERSAL DE MODELOS
========================================

Este script testa o sistema universal de modelos para garantir que tudo estÃ¡ funcionando.
"""

import sys
import os
from pathlib import Path

# Adiciona o diretÃ³rio do projeto ao path
project_dir = Path(__file__).parent
sys.path.append(str(project_dir))

def test_universal_config():
    """Testa se o sistema universal estÃ¡ funcionando"""
    print("ğŸ§ª TESTANDO SISTEMA UNIVERSAL...")
    
    try:
        from config.universal_model_config import (
            get_universal_config,
            get_available_models, 
            detect_model_type,
            get_performance_info_universal
        )
        
        print("âœ… ImportaÃ§Ãµes universais OK")
        
        # Testa detecÃ§Ã£o de modelo
        test_models = ['codellama:7b', 'deepseek-r1:14b', 'mistral:7b']
        
        for model in test_models:
            model_type = detect_model_type(model)
            config = get_universal_config(model, 'fast')
            
            print(f"  ğŸ“Š {model} -> Tipo: {model_type}")
            print(f"     Temp: {config.get('temperature', 'N/A')}, Ctx: {config.get('num_ctx', 'N/A')}")
        
        # Testa modelos disponÃ­veis
        available = get_available_models()
        print(f"âœ… {len(available)} modelos disponÃ­veis no sistema")
        
        # Testa informaÃ§Ãµes de performance
        perf_info = get_performance_info_universal()
        print(f"âœ… {len(perf_info['performance_modes'])} modos de performance")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no sistema universal: {e}")
        return False

def test_prompt_manager():
    """Testa se o PromptManager estÃ¡ funcionando"""
    print("\\nğŸ§ª TESTANDO PROMPT MANAGER...")
    
    try:
        from prompts.specialized_prompts import PromptManager
        
        # Testa inicializaÃ§Ã£o
        pm = PromptManager(performance_mode='fast', model_name='codellama:7b')
        print("âœ… PromptManager inicializado")
        
        # Testa mudanÃ§a de modelo
        pm.set_model('mistral:7b')
        pm.set_performance_mode('balanced')
        print("âœ… MudanÃ§a de modelo/performance OK")
        
        # Testa geraÃ§Ã£o de prompt
        try:
            analysis_prompt = pm.get_analysis_prompt()
            if analysis_prompt:
                print(f"âœ… Prompt gerado: {len(analysis_prompt)} caracteres")
            else:
                print("âš ï¸ Prompt vazio, mas sem erro")
        except Exception as e:
            print(f"âš ï¸ Erro ao gerar prompt: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no PromptManager: {e}")
        return False

def test_llm_service():
    """Testa se o LLMService estÃ¡ funcionando"""
    print("\\nğŸ§ª TESTANDO LLM SERVICE...")
    
    try:
        from core.llm_service import LLMService
        
        # ConfiguraÃ§Ã£o bÃ¡sica de teste
        config = {
            'ollama_model': 'codellama:7b',
            'performance_mode': 'fast',
            'groq_api_key': ''
        }
        
        llm = LLMService(config)
        print("âœ… LLMService inicializado")
        
        # Verifica se configuraÃ§Ãµes foram aplicadas
        if 'temperature' in llm.config:
            print(f"âœ… ConfiguraÃ§Ãµes universais aplicadas: temp={llm.config.get('temperature', 'N/A')}")
        else:
            print("âš ï¸ ConfiguraÃ§Ãµes universais nÃ£o detectadas")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro no LLMService: {e}")
        return False

def test_interface_compatibility():
    """Testa se a interface estÃ¡ compatÃ­vel"""
    print("\\nğŸ§ª TESTANDO INTERFACE...")
    
    try:
        # Simula importaÃ§Ã£o da interface
        import streamlit as st
        
        # Testa se as funÃ§Ãµes principais existem
        from ui.interface import JUNIMInterface
        
        interface = JUNIMInterface()
        print("âœ… Interface importada com sucesso")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro na interface: {e}")
        return False

def main():
    """Executa todos os testes"""
    print("ğŸš€ INICIANDO TESTES DO SISTEMA UNIVERSAL\\n")
    
    tests = [
        ("Sistema Universal", test_universal_config),
        ("Prompt Manager", test_prompt_manager), 
        ("LLM Service", test_llm_service),
        ("Interface", test_interface_compatibility)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ FALHA CRÃTICA em {test_name}: {e}")
            results.append((test_name, False))
    
    # Resumo
    print("\\n" + "="*50)
    print("ğŸ“Š RESUMO DOS TESTES")
    print("="*50)
    
    passed = 0
    for test_name, result in results:
        status = "âœ… PASSOU" if result else "âŒ FALHOU"
        print(f"{status}: {test_name}")
        if result:
            passed += 1
    
    print(f"\\nğŸ¯ RESULTADO: {passed}/{len(results)} testes passaram")
    
    if passed == len(results):
        print("\\nğŸ‰ TODOS OS TESTES PASSARAM!")
        print("âœ… Sistema universal funcionando perfeitamente")
        print("ğŸš€ Pronto para usar com codellama:7b!")
    else:
        print("\\nâš ï¸ ALGUNS TESTES FALHARAM")
        print("ğŸ”§ Verifique as dependÃªncias e configuraÃ§Ãµes")
    
    return passed == len(results)

if __name__ == "__main__":
    success = main()
    
    if success:
        print("\\n" + "="*50)
        print("ğŸ¯ PRÃ“XIMOS PASSOS:")
        print("="*50)
        print("1. âœ… Execute: ollama pull codellama:7b")
        print("2. âœ… Execute: streamlit run main.py")
        print("3. âœ… Configure modelo na sidebar")
        print("4. âœ… Teste com modo 'FAST' para velocidade!")
        print("\\nğŸš€ Aproveite o sistema 3x mais rÃ¡pido!")
    else:
        print("\\nğŸ”§ Execute este teste novamente apÃ³s corrigir os problemas.")
